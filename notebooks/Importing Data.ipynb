{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
       "\n",
       "// needed to generate the Table of contents \n",
       "// taken from github.com/kmahelona/ipython_notebook_goodies\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')\n",
    "\n",
    "// needed to generate the Table of contents \n",
    "// taken from github.com/kmahelona/ipython_notebook_goodies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(To work with Atom and Jupyter at the same time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "### Locating the data\n",
    "\n",
    "To train this model we will use the Turbofan Engine Degradation Simulation Data Set from NASA ([Link to dataset](https://ti.arc.nasa.gov/c/6/)).\n",
    "\n",
    ">A. Saxena and K. Goebel (2008). \"Turbofan Engine Degradation Simulation Data Set\", NASA Ames Prognostics Data Repository (http://ti.arc.nasa.gov/project/prognostic-data-repository), NASA Ames Research Center, Moffett Field, CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load python packages\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "#import datetime\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrea/Dropbox/PyProjects/Predictive_Maintenance_Fanjet\n"
     ]
    }
   ],
   "source": [
    "#navigate to the data folder\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/raw’: File exists\n",
      "--2020-05-29 16:11:12--  https://ti.arc.nasa.gov/c/6/\n",
      "Resolving ti.arc.nasa.gov (ti.arc.nasa.gov)... 128.102.105.66, 2001:4d0:6311:2227:14b6:372b:2078:2a94\n",
      "Connecting to ti.arc.nasa.gov (ti.arc.nasa.gov)|128.102.105.66|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 FOUND\n",
      "Location: https://ti.arc.nasa.gov/m/project/prognostic-repository/CMAPSSData.zip [following]\n",
      "--2020-05-29 16:11:12--  https://ti.arc.nasa.gov/m/project/prognostic-repository/CMAPSSData.zip\n",
      "Reusing existing connection to ti.arc.nasa.gov:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12425978 (12M) [application/zip]\n",
      "Saving to: ‘turbofan.zip’\n",
      "\n",
      "turbofan.zip        100%[===================>]  11.85M  8.03MB/s    in 1.5s    \n",
      "\n",
      "2020-05-29 16:11:14 (8.03 MB/s) - ‘turbofan.zip’ saved [12425978/12425978]\n",
      "\n",
      "Archive:  turbofan.zip\n",
      "  inflating: Damage Propagation Modeling.pdf  \n",
      "  inflating: readme.txt              \n",
      "  inflating: RUL_FD001.txt           \n",
      "  inflating: RUL_FD002.txt           \n",
      "  inflating: RUL_FD003.txt           \n",
      "  inflating: RUL_FD004.txt           \n",
      "  inflating: test_FD001.txt          \n",
      "  inflating: test_FD002.txt          \n",
      "  inflating: test_FD003.txt          \n",
      "  inflating: test_FD004.txt          \n",
      "  inflating: train_FD001.txt         \n",
      "  inflating: train_FD002.txt         \n",
      "  inflating: train_FD003.txt         \n",
      "  inflating: train_FD004.txt         \n"
     ]
    }
   ],
   "source": [
    "# Download data in data/raw folder\n",
    "! mkdir data/raw\n",
    "! cd data/raw && wget -O turbofan.zip https://ti.arc.nasa.gov/c/6/ && unzip -o *.zip\n",
    "! cd data/raw && rm *.zip\n",
    "\n",
    "#NOTE the cd executed with the magic commanda do not change the working directory of the python script\n",
    "#We are still working off the project root folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "After downloading and unzipping the files, the data folder has the following structure. \n",
    "\n",
    "```\n",
    "data/raw\n",
    "├── Damage Propagation Modeling.pdf\n",
    "├── readme.txt\n",
    "├── RUL_FD001.txt\n",
    "├── RUL_FD002.txt\n",
    "├── RUL_FD003.txt\n",
    "├── RUL_FD004.txt\n",
    "├── test_FD001.txt\n",
    "├── test_FD002.txt\n",
    "├── test_FD003.txt\n",
    "├── test_FD004.txt\n",
    "├── train_FD001.txt\n",
    "├── train_FD002.txt\n",
    "├── train_FD003.txt\n",
    "└── train_FD004.txt\n",
    "```\n",
    "\n",
    "From the readme file we can extract the file structure. There are 4 sets of files:\n",
    "1. FD001\n",
    "2. FD002\n",
    "3. FD003\n",
    "4. FD004\n",
    "\n",
    "\n",
    "Each containing 3 types of files:\n",
    "1. Training data\n",
    "2. Test data\n",
    "3. Remaining Usable Life (RUL) data\n",
    "\n",
    "Test and training data files have the same column structure:\n",
    "1. unit number\n",
    "2. cycle time\n",
    "3. operating setting 1\n",
    "4. operating setting 2\n",
    "5. operating setting 3\n",
    "6. sensor reading 1\n",
    "7. sensor reading 2\n",
    "...\n",
    "8. sensor reading 21\n",
    "\n",
    "\n",
    "The RUL data file has a single column corresponding to the RUL value.\n",
    "\n",
    "We will import the txt files and consolidate the data in 3 labelled csv files which will reside in the data folder:\n",
    "```\n",
    "data\n",
    "├── RUL.csv\n",
    "├── test.csv\n",
    "└── train.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names\n",
    "sensors_list = [\"s{}\".format(s) for s in range(1,22)]\n",
    "train_cols = ['unit_number','cycle_time','op_setting_1', 'op_setting_2', 'op_setting_3'] + sensors_list\n",
    "test_cols = train_cols\n",
    "RUL_cols = ['RUL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load Training data\n",
    "file_paths = glob.glob(\"data/raw/train_*.txt\")\n",
    "\n",
    "df_list = []\n",
    "for file_path in file_paths : \n",
    "    # read txt file\n",
    "    individual_df = pd.read_csv(file_path, sep=' ', header=None, usecols = [i for i in range(26)])\n",
    "    individual_df.columns = train_cols\n",
    "    #extract dataset Id from filename\n",
    "    data_set=file_path[-9:-4]\n",
    "    individual_df['dataset']=file_path[-9:-4] \n",
    "    #append temporary dataframe to list\n",
    "    df_list.append(individual_df)\n",
    "#merge into single dataframe\n",
    "df_train=pd.concat(df_list)\n",
    "df_train.head()\n",
    "\n",
    "# write to csv\n",
    "df_train.to_csv('data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Test data\n",
    "file_paths = glob.glob(\"data/raw/test_*.txt\")\n",
    "\n",
    "df_list = []\n",
    "for file_path in file_paths : \n",
    "    # read txt file\n",
    "    individual_df = pd.read_csv(file_path, sep=' ', header=None, usecols = [i for i in range(26)])\n",
    "    individual_df.columns = test_cols\n",
    "    #extract dataset Id from filename\n",
    "    data_set=file_path[-9:-4]\n",
    "    individual_df['dataset']=file_path[-9:-4] \n",
    "    #append temporary dataframe to list\n",
    "    df_list.append(individual_df)\n",
    "#merge into single dataframe\n",
    "df_test=pd.concat(df_list)\n",
    "df_test.head()\n",
    "\n",
    "# write to csv\n",
    "df_test.to_csv('data/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load RUL data\n",
    "file_paths = glob.glob(\"data/raw/RUL_*.txt\")\n",
    "\n",
    "df_list = []\n",
    "for file_path in file_paths : \n",
    "    # read txt file\n",
    "    individual_df = pd.read_csv(file_path, sep=' ', header=None, usecols=[0])\n",
    "    individual_df.columns = RUL_cols\n",
    "    #extract dataset Id from filename\n",
    "    data_set=file_path[-9:-4]\n",
    "    individual_df['dataset']=file_path[-9:-4] \n",
    "    #append temporary dataframe to list\n",
    "    df_list.append(individual_df)\n",
    "#merge into single dataframe\n",
    "df_RUL=pd.concat(df_list)\n",
    "df_RUL.head()\n",
    "\n",
    "# write to csv\n",
    "df_RUL.to_csv('data/RUL.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Jet]",
   "language": "python",
   "name": "conda-env-Jet-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
